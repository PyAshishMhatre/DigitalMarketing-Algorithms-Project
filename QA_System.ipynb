{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0UFuDnl6akdhW55/ljIg9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PyAshishMhatre/DigitalMarketing-Algorithms-Project/blob/main/QA_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Youtube video transcription"
      ],
      "metadata": {
        "id": "nIILJF_gJlmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Libraries\n",
        "!pip install pytube -q\n",
        "!pip install git+https://github.com/openai/whisper.git -"
      ],
      "metadata": {
        "id": "2vmhb5xbNkS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing modules for transcriptions\n",
        "\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "djaTgz__Nj7t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xp99kaHxIIFd"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model('tiny')\n",
        "\n",
        "# Function to download video and get transcriptions\n",
        "def get_transcriptions(url, model):\n",
        "    yt_video = YouTube(url)\n",
        "    stream = yt_video.streams.filter(only_audio=True)\n",
        "    stream = stream.first()\n",
        "    stream.download(filename=\"test.mp4\")\n",
        "    output = model.transcribe(\"test.mp4\")\n",
        "\n",
        "    filename = \"test.txt\" # the name of the file to be saved\n",
        "\n",
        "    with open(filename, \"w\") as file:\n",
        "      file.write(output[\"text\"])\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List down the youtube videos for transcription\n",
        "\n",
        "video_urls = [\"https://www.youtube.com/watch?v=blbvVUxD41Q&ab_channel=Locust%26WildHoney\",\n",
        "              \"https://www.youtube.com/watch?v=9hktZEc3Vhs&ab_channel=STYLEDBYNATE\",\n",
        "              \"https://www.youtube.com/watch?v=C9nVeYwS_8E&ab_channel=Men%27sFashionFiles\",\n",
        "              \"https://www.youtube.com/watch?v=7fJcrPjAa1I&ab_channel=UniqloReviews\",\n",
        "              \"https://www.youtube.com/watch?v=YtjHtPySBAA&ab_channel=HarryHas\"]\n",
        "\n",
        "# Loop through each video URL and get transcriptions\n",
        "for url in tqdm(video_urls):\n",
        "  get_transcriptions(url, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rupXLnvIjjg",
        "outputId": "390a0fb3-a1a6-4bb3-8157-7ffa9afe1f7d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "100%|██████████| 1/1 [01:19<00:00, 79.16s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing QA system using Augmented Retrival and prompt engineering using Langchain and ChromaDB (Vector Database)"
      ],
      "metadata": {
        "id": "CppEVRm-JZgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0JFXLnLR3iN",
        "outputId": "9c84d169-204f-4d59-d3fe-f0d9a864a6f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.28.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import langchain modules \n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from typing import List\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document\n",
        "import os\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "22F3YU68NYM0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "Kh5qbOFBSHhF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text files of transcription from directory \n",
        "\n",
        "loader = DirectoryLoader('./Documents', glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True)"
      ],
      "metadata": {
        "id": "2OhteJdfgGNr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load usind load()\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTlI57y1gZS3",
        "outputId": "910a0e28-2d76-4237-fe11-7413d81c6a2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 4467.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking length of loaded files\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0M98EqxgcfV",
        "outputId": "d0763408-ce9a-4fd5-873b-c08ac4d82989"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into small chunks for more efficient context retrival\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "_SbspdJWgdWN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the new document length after text splitting\n",
        "\n",
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZJ2ZID_Kwtp",
        "outputId": "63f5df59-29c6-4d9c-9b62-fc8efea5b24e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "771"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading vectors into Chroma DB\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwUQa-TAh0vx",
        "outputId": "fc798bf9-2d74-4fa3-bd7e-6c80714fc4ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.3, openai_api_key=\"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\")"
      ],
      "metadata": {
        "id": "qpfwhfX9xT62"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the quality of tshirt?\""
      ],
      "metadata": {
        "id": "KWFSb85Jzohy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = vectordb.similarity_search(query, k=8)"
      ],
      "metadata": {
        "id": "Z122uh-IwkZd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Dkbl6YpNEB9",
        "outputId": "848e4af2-9ebd-4056-d0fa-0269d1ee9626"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"t-shirts have been very, very durable in the wash so that's definitely something. A worth mentioning\", metadata={'source': 'Documents/test1.txt'}),\n",
              " Document(page_content=\"improved on the T-shirt, but it wasn't back to the standards that it used to be before they changed\", metadata={'source': 'Documents/test4.txt'}),\n",
              " Document(page_content=\"improved on the T-shirt, but it wasn't back to the standards that it used to be before they changed\", metadata={'source': 'Documents/Is UNIQLO Still The Best For Affordable Basics_.txt'}),\n",
              " Document(page_content=\"they're any good, just want to mention some points about these shirts and my experience with them.\", metadata={'source': 'Documents/Are Uniqlo shirts any good_ Owner’s review.txt'}),\n",
              " Document(page_content='still sell the best quality affordable basics, or are you better off spending your money elsewhere?', metadata={'source': 'Documents/test4.txt'}),\n",
              " Document(page_content='still sell the best quality affordable basics, or are you better off spending your money elsewhere?', metadata={'source': 'Documents/Is UNIQLO Still The Best For Affordable Basics_.txt'}),\n",
              " Document(page_content='very soft and the material quality is very good. I wore this shirt maybe like a few times, maybe', metadata={'source': 'Documents/Are Uniqlo shirts any good_ Owner’s review.txt'}),\n",
              " Document(page_content='great T-shirt for the price. They then changed the fit for the worse, and the quality did go', metadata={'source': 'Documents/Is UNIQLO Still The Best For Affordable Basics_.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = \"\"\" Prompt: Use the following pieces of context to answer the question at the end and Answer as if your a salesman of the company\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "LCOIy3PH0f7L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Here is a statement:\n",
        "        {statement}\n",
        "        Make the statement sound like it is coming from a customer care representative at ABC clothing company.\\n\\n\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"statement\"], template=template)"
      ],
      "metadata": {
        "id": "ixsNufk6SpLL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.llm import LLMChain\n",
        "question_chain = LLMChain(llm=llm, prompt=PROMPT , output_key = 'statement')\n"
      ],
      "metadata": {
        "id": "19H6qR3T6sPH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "assumptions_chain = LLMChain(llm=llm, prompt=prompt_template, output_key = 'response')\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[question_chain, assumptions_chain],\n",
        "    input_variables=['context', \"question\"],\n",
        "    # Here we return multiple variables\n",
        "    output_variables=[\"statement\", \"response\"],\n",
        "    verbose=False)"
      ],
      "metadata": {
        "id": "phiPue__O6fY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain({'context':docsearch, 'question':query})['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "l3PlyZOzTKLU",
        "outputId": "8dab8d08-bea2-4e69-b8f4-e5a1a40df819"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'At ABC Clothing Company, we take pride in the quality of our t-shirts. Our customers have praised their durability in the wash and their softness. We are continuously striving to improve the quality of our t-shirts, and we are confident that they will soon be back to the standards they used to be. We are proud of the material quality and the fit of our t-shirts, especially for the price.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yffpTVT5vYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}